{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is Simple Linear Regression?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a concise and clear explanation of **Simple Linear Regression (SLR):**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Definition**\n",
        "\n",
        "**Simple Linear Regression** is a **statistical and machine learning technique** used to model the **linear relationship** between:\n",
        "\n",
        "* **One independent variable (X)** – the predictor\n",
        "* **One dependent variable (Y)** – the outcome\n",
        "\n",
        "It fits a straight line through the data to **predict Y based on X**.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Mathematical Form**\n",
        "\n",
        "[\n",
        "Y = \\beta_0 + \\beta_1 X + \\varepsilon\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (Y) = dependent variable (what you want to predict)\n",
        "* (X) = independent variable (predictor)\n",
        "* (\\beta_0) = intercept (value of Y when X = 0)\n",
        "* (\\beta_1) = slope (change in Y for a one-unit change in X)\n",
        "* (\\varepsilon) = error term (captures random noise)\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Purpose**\n",
        "\n",
        "* **Prediction:** Estimate the value of Y for new X values.\n",
        "* **Understanding relationships:** Quantify how X affects Y.\n",
        "* **Trend analysis:** Identify linear trends in data.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Example**\n",
        "\n",
        "Predicting **sales (Y)** based on **advertising budget (X)**:\n",
        "\n",
        "[\n",
        "\\text{Sales} = 50 + 3 \\cdot \\text{Advertising Budget}\n",
        "]\n",
        "\n",
        "* Intercept (50): Base sales if advertising = $0\n",
        "* Slope (3): Each $1 increase in advertising increases sales by $3\n",
        "\n",
        "---\n",
        "\n",
        "### **One-Line Summary**\n",
        "\n",
        "> Simple Linear Regression models a **straight-line relationship between one predictor and a continuous outcome**, allowing prediction and understanding of the effect of X on Y.\n",
        "\n"
      ],
      "metadata": {
        "id": "PcAOeJ1PhDgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a clear breakdown of the **key assumptions of Simple Linear Regression (SLR)**:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Linearity**\n",
        "\n",
        "* **Assumption:** There is a **linear relationship** between the independent variable (X) and the dependent variable (Y).\n",
        "* **Implication:** The regression line accurately represents the trend in the data.\n",
        "* **Violation:** If the relationship is non-linear, predictions will be biased.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Independence of Errors**\n",
        "\n",
        "* **Assumption:** The residuals (errors) (\\varepsilon_i = Y_i - \\hat{Y}_i) are **independent** of each other.\n",
        "* **Implication:** Observations are not correlated, which is important especially in time-series data.\n",
        "* **Violation:** Autocorrelation can lead to underestimated standard errors and unreliable hypothesis tests.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Homoscedasticity (Constant Variance)**\n",
        "\n",
        "* **Assumption:** The variance of residuals is **constant** across all values of (X).\n",
        "* **Implication:** The spread of errors should be roughly the same for all predicted values.\n",
        "* **Violation:** Heteroscedasticity (errors with non-constant variance) can make confidence intervals and predictions unreliable.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Normality of Errors**\n",
        "\n",
        "* **Assumption:** The residuals (\\varepsilon) are **normally distributed**.\n",
        "* **Implication:** Required for valid **hypothesis testing** and confidence intervals.\n",
        "* **Note:** Not strictly necessary for prediction if the dataset is large (Central Limit Theorem).\n",
        "\n",
        "---\n",
        "\n",
        "## **5. No Multicollinearity**\n",
        "\n",
        "* **Assumption:** Relevant for multiple linear regression (not SLR), meaning predictors should not be highly correlated.\n",
        "* **In SLR:** Automatically satisfied since there is only **one predictor**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary Table**\n",
        "\n",
        "| Assumption           | Meaning                           | Why Important                            |\n",
        "| -------------------- | --------------------------------- | ---------------------------------------- |\n",
        "| Linearity            | Y changes linearly with X         | Ensures model fits correctly             |\n",
        "| Independence         | Errors are independent            | Valid standard errors and tests          |\n",
        "| Homoscedasticity     | Constant error variance           | Reliable confidence intervals            |\n",
        "| Normality            | Errors follow normal distribution | Needed for inference                     |\n",
        "| No Multicollinearity | Predictors not highly correlated  | Ensures coefficient estimates are stable |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "a2RM7vYahUzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is heteroscedasticity, and why is it important to address in regression\n",
        "models?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a clear explanation of **heteroscedasticity** and why it matters in regression:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Definition**\n",
        "\n",
        "**Heteroscedasticity** occurs when the **variance of the errors (residuals) is not constant** across all levels of the independent variable(s).\n",
        "\n",
        "* In other words, the spread of residuals **changes** depending on the value of (X) or the predicted (Y).\n",
        "* **Opposite:** Homoscedasticity → residuals have constant variance.\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "[\n",
        "\\text{Var}(\\varepsilon_i) \\neq \\text{constant for all } i\n",
        "]\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Why It Happens**\n",
        "\n",
        "* Model misses important variables\n",
        "* Non-linear relationships\n",
        "* Outliers or extreme values\n",
        "* Unequal measurement precision\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Why It Is Important**\n",
        "\n",
        "1. **Bias in Standard Errors:**\n",
        "\n",
        "   * Heteroscedasticity does **not bias coefficient estimates** ((\\beta_0, \\beta_1)), but it **inflates or deflates standard errors**.\n",
        "   * This leads to **incorrect confidence intervals and p-values**, making hypothesis tests unreliable.\n",
        "\n",
        "2. **Misleading Model Fit:**\n",
        "\n",
        "   * The model may appear to fit well in some regions of X but poorly in others.\n",
        "\n",
        "3. **Prediction Accuracy:**\n",
        "\n",
        "   * Predictions in regions with higher variance will be less reliable.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. How to Detect Heteroscedasticity**\n",
        "\n",
        "* **Visual:** Plot residuals vs predicted values; look for **funnel shapes** or patterns.\n",
        "* **Statistical Tests:**\n",
        "\n",
        "  * **Breusch-Pagan Test**\n",
        "  * **White Test**\n",
        "\n",
        "---\n",
        "\n",
        "## **5. How to Address It**\n",
        "\n",
        "* Transform the dependent variable (e.g., log, square root)\n",
        "* Use **weighted least squares regression**\n",
        "* Use **robust standard errors**\n",
        "* Consider **non-linear models** if appropriate\n",
        "\n",
        "---\n",
        "\n",
        "### **Intuition**\n",
        "\n",
        "> Ideally, residuals should be like **a “cloud” evenly spread** around zero across all X values.\n",
        "> Heteroscedasticity is like a **fanning or funnel shape**, meaning the model’s errors grow or shrink systematically with X.\n",
        "\n"
      ],
      "metadata": {
        "id": "z2OCNLCXhmD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  What is Multiple Linear Regression?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a clear explanation of **Multiple Linear Regression (MLR):**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Definition**\n",
        "\n",
        "**Multiple Linear Regression** is a **statistical and machine learning technique** used to model the **linear relationship between one dependent variable (Y) and two or more independent variables (X₁, X₂, …, Xₙ)**.\n",
        "\n",
        "* Extends **Simple Linear Regression (SLR)** from one predictor to multiple predictors.\n",
        "* Used when the outcome depends on **several factors**.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Mathematical Form**\n",
        "\n",
        "[\n",
        "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n + \\varepsilon\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (Y) = dependent variable (target)\n",
        "* (X_1, X_2, \\dots, X_n) = independent variables (features)\n",
        "* (\\beta_0) = intercept (value of Y when all X’s = 0)\n",
        "* (\\beta_1, \\beta_2, …, \\beta_n) = coefficients (effect of each predictor on Y)\n",
        "* (\\varepsilon) = error term (random noise)\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Purpose**\n",
        "\n",
        "1. **Prediction:** Estimate the value of Y based on multiple predictors.\n",
        "2. **Understanding Relationships:** Measure the effect of each predictor on Y while **controlling for other variables**.\n",
        "3. **Decision-Making:** Helps identify **key drivers** affecting the outcome.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Example**\n",
        "\n",
        "**Problem:** Predict house price based on multiple features:\n",
        "\n",
        "[\n",
        "\\text{Price} = 50,000 + 100 \\cdot \\text{Area} + 20,000 \\cdot \\text{Bedrooms} - 5,000 \\cdot \\text{Age} + \\varepsilon\n",
        "]\n",
        "\n",
        "* **Intercept (50,000):** Base price\n",
        "* **Slope coefficients:**\n",
        "\n",
        "  * Area → Price increases $100 per sq ft\n",
        "  * Bedrooms → Price increases $20,000 per bedroom\n",
        "  * Age → Price decreases $5,000 per year\n",
        "\n",
        "---\n",
        "\n",
        "### **One-Line Summary**\n",
        "\n",
        "> **Multiple Linear Regression models the relationship between a dependent variable and multiple independent variables, allowing both prediction and interpretation of individual effects.**\n",
        "\n"
      ],
      "metadata": {
        "id": "SgedxImMiJj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is polynomial regression, and how does it differ from linear\n",
        "regression?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a clear explanation of **Polynomial Regression** and how it differs from Linear Regression:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. What is Polynomial Regression?**\n",
        "\n",
        "**Polynomial Regression** is a type of regression analysis where the relationship between the **independent variable(s) (X)** and the **dependent variable (Y)** is modeled as an **n-th degree polynomial** rather than a straight line.\n",
        "\n",
        "* Useful when the data shows a **curved (non-linear) trend**.\n",
        "* Can be seen as an extension of **Simple or Multiple Linear Regression** by adding polynomial terms like (X^2, X^3), etc.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Mathematical Form**\n",
        "\n",
        "For a **single feature** (X) and a polynomial of degree (d):\n",
        "\n",
        "[\n",
        "Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\dots + \\beta_d X^d + \\varepsilon\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (Y) = dependent variable\n",
        "* (X) = independent variable\n",
        "* (\\beta_0, \\beta_1, ..., \\beta_d) = coefficients\n",
        "* (\\varepsilon) = error term\n",
        "\n",
        "---\n",
        "\n",
        "## **3. How It Differs from Linear Regression**\n",
        "\n",
        "| Feature          | Linear Regression                       | Polynomial Regression                                                       |\n",
        "| ---------------- | --------------------------------------- | --------------------------------------------------------------------------- |\n",
        "| Relationship     | Linear: (Y) changes linearly with X     | Non-linear: (Y) changes according to a polynomial of X                      |\n",
        "| Equation         | (Y = \\beta_0 + \\beta_1 X + \\varepsilon) | (Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\dots + \\beta_d X^d + \\varepsilon) |\n",
        "| Curve            | Straight line                           | Curved line (parabola, cubic, etc.)                                         |\n",
        "| Use Case         | Linear trends                           | Non-linear trends where straight line is a poor fit                         |\n",
        "| Model Complexity | Simple                                  | Higher (risk of overfitting if degree is too high)                          |\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Example**\n",
        "\n",
        "Suppose you are predicting **car speed based on engine power**, and the data shows a curved trend:\n",
        "\n",
        "* Linear Regression: ( \\text{Speed} = 20 + 0.5 \\cdot \\text{Power} ) → straight line, poor fit\n",
        "* Polynomial Regression (degree 2): ( \\text{Speed} = 10 + 0.8 \\cdot \\text{Power} - 0.002 \\cdot \\text{Power}^2 ) → curved line fits better\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Intuition**\n",
        "\n",
        "* **Linear Regression:** “Draw the best straight line through the points.”\n",
        "* **Polynomial Regression:** “Fit a smooth curve that can bend to capture non-linear patterns.”\n",
        "\n"
      ],
      "metadata": {
        "id": "AaCICgx8ibP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "6.  Implement a Python program to fit a Simple Linear Regression model to\n",
        "the following sample data:\n",
        "● X = [1, 2, 3, 4, 5]\n",
        "● Y = [2.1, 4.3, 6.1, 7.9, 10.2]\n",
        "Plot the regression line over the data points.\n",
        "\n",
        "\n",
        "ANS-\n",
        "\n",
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Reshape for sklearn\n",
        "Y = np.array([2.1, 4.3, 6.1, 7.9, 10.2])\n",
        "\n",
        "# Create and fit the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# Get slope and intercept\n",
        "slope = model.coef_[0]\n",
        "intercept = model.intercept_\n",
        "print(f\"Slope (beta_1): {slope:.4f}\")\n",
        "print(f\"Intercept (beta_0): {intercept:.4f}\")\n",
        "\n",
        "# Predict Y values using the model\n",
        "Y_pred = model.predict(X)\n",
        "\n",
        "# Plot the data points and regression line\n",
        "plt.scatter(X, Y, color='blue', label='Data points')\n",
        "plt.plot(X, Y_pred, color='red', linewidth=2, label='Regression line')\n",
        "plt.title(\"Simple Linear Regression\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xLvlgRWCiw4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7.  Fit a Multiple Linear Regression model on this sample data:\n",
        "● Area = [1200, 1500, 1800, 2000]\n",
        "● Rooms = [2, 3, 3, 4]\n",
        "● Price = [250000, 300000, 320000, 370000]\n",
        "Check for multicollinearity using VIF and report the results.\n",
        "\n",
        "ANS-\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Area': [1200, 1500, 1800, 2000],\n",
        "    'Rooms': [2, 3, 3, 4],\n",
        "    'Price': [250000, 300000, 320000, 370000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Independent variables\n",
        "X = df[['Area', 'Rooms']]\n",
        "y = df['Price']\n",
        "\n",
        "# Fit the Multiple Linear Regression model\n",
        "mlr = LinearRegression()\n",
        "mlr.fit(X, y)\n",
        "\n",
        "# Print coefficients\n",
        "print(\"Intercept:\", mlr.intercept_)\n",
        "print(\"Coefficients:\", dict(zip(X.columns, mlr.coef_)))\n",
        "\n",
        "# -----------------------\n",
        "# Check for multicollinearity using VIF\n",
        "# -----------------------\n",
        "# Add a constant for statsmodels\n",
        "X_const = sm.add_constant(X)\n",
        "\n",
        "# Calculate VIF for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['Feature'] = X.columns\n",
        "vif_data['VIF'] = [variance_inflation_factor(X_const.values, i+1) for i in range(X.shape[1])]\n",
        "\n",
        "print(\"\\nVariance Inflation Factor (VIF):\")\n",
        "print(vif_data)\n"
      ],
      "metadata": {
        "id": "WIkXp5Cti5Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. Implement polynomial regression on the following data:\n",
        "● X = [1, 2, 3, 4, 5]\n",
        "3\n",
        "● Y = [2.2, 4.8, 7.5, 11.2, 14.7]\n",
        "Fit a 2nd-degree polynomial and plot the resulting curve.\n",
        "\n",
        "\n",
        "ANS-\n",
        "\n",
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "Y = np.array([2.2, 4.8, 7.5, 11.2, 14.7])\n",
        "\n",
        "# Transform features to polynomial (degree 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit linear regression on transformed features\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, Y)\n",
        "\n",
        "# Print coefficients\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "\n",
        "# Predict values for plotting\n",
        "X_fit = np.linspace(1, 5, 100).reshape(-1, 1)\n",
        "X_fit_poly = poly.transform(X_fit)\n",
        "Y_fit = model.predict(X_fit_poly)\n",
        "\n",
        "# Plot original data and polynomial curve\n",
        "plt.scatter(X, Y, color='blue', label='Data points')\n",
        "plt.plot(X_fit, Y_fit, color='red', linewidth=2, label='Polynomial Regression (degree 2)')\n",
        "plt.title(\"Polynomial Regression (2nd Degree)\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QGySfilrjMGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9. Create a residuals plot for a regression model trained on this data:\n",
        "● X = [10, 20, 30, 40, 50]\n",
        "● Y = [15, 35, 40, 50, 65]\n",
        "Assess heteroscedasticity by examining the spread of residuals.\n",
        "\n",
        "\n",
        "ANS-\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = np.array([10, 20, 30, 40, 50]).reshape(-1, 1)\n",
        "Y = np.array([15, 35, 40, 50, 65])\n",
        "\n",
        "# Fit a Simple Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# Predict Y values\n",
        "Y_pred = model.predict(X)\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = Y - Y_pred\n",
        "\n",
        "# Print model coefficients\n",
        "print(f\"Intercept: {model.intercept_:.2f}\")\n",
        "print(f\"Slope: {model.coef_[0]:.2f}\")\n",
        "\n",
        "# Plot residuals\n",
        "plt.scatter(Y_pred, residuals, color='blue')\n",
        "plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
        "plt.title(\"Residuals Plot\")\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_eYriMDPjWi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you are a data scientist working for a real estate company. You\n",
        "need to predict house prices using features like area, number of rooms, and location.\n",
        "However, you detect heteroscedasticity and multicollinearity in your regression\n",
        "model. Explain the steps you would take to address these issues and ensure a robust\n",
        "model.\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a **structured approach** to handle **heteroscedasticity** and **multicollinearity** in a real-world real estate regression problem:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Detect the Problems**\n",
        "\n",
        "* **Heteroscedasticity:** Residuals have non-constant variance (e.g., larger houses show larger errors).\n",
        "\n",
        "  * Detected via:\n",
        "\n",
        "    * Residuals vs predicted plot (funnel shape)\n",
        "    * Breusch-Pagan or White test\n",
        "\n",
        "* **Multicollinearity:** Predictors are highly correlated (e.g., area and number of rooms).\n",
        "\n",
        "  * Detected via:\n",
        "\n",
        "    * Variance Inflation Factor (VIF > 10 indicates high correlation)\n",
        "    * Correlation matrix heatmap\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Address Heteroscedasticity**\n",
        "\n",
        "* **Transform the dependent variable (Y):**\n",
        "\n",
        "  * Apply **log, square root, or Box-Cox transformation** to stabilize variance.\n",
        "  * Example: `Y_transformed = np.log(Price)`\n",
        "\n",
        "* **Weighted Least Squares (WLS):**\n",
        "\n",
        "  * Give lower weights to observations with higher variance.\n",
        "\n",
        "* **Robust Regression:**\n",
        "\n",
        "  * Use models that are less sensitive to heteroscedasticity, e.g., `statsmodels`’ `RLM` or `sklearn`’s `HuberRegressor`.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Address Multicollinearity**\n",
        "\n",
        "* **Remove or combine correlated features:**\n",
        "\n",
        "  * Drop one of the highly correlated variables (e.g., choose between area or number of rooms).\n",
        "  * Combine features (e.g., `price_per_room = Price / Rooms`).\n",
        "\n",
        "* **Regularization techniques:**\n",
        "\n",
        "  * **Ridge Regression:** Penalizes large coefficients and reduces multicollinearity effects.\n",
        "  * **Lasso Regression:** Can shrink some coefficients to zero, performing feature selection.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Feature Engineering**\n",
        "\n",
        "* Encode categorical variables like location properly:\n",
        "\n",
        "  * **One-Hot Encoding** or **Target Encoding** (if many locations).\n",
        "\n",
        "* Normalize or standardize numeric features if using regularized regression.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Model Validation**\n",
        "\n",
        "* **Cross-validation:** Use k-fold CV to ensure the model generalizes well.\n",
        "* **Metrics:** Track RMSE, MAE, and R-squared.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Optional: Use Tree-Based Models**\n",
        "\n",
        "* If linear assumptions are difficult to satisfy, consider:\n",
        "\n",
        "  * **Random Forest Regressor** or **Gradient Boosting**\n",
        "  * These **handle non-linearity, multicollinearity, and heteroscedasticity** naturally.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Justification to Stakeholders**\n",
        "\n",
        "* By **transforming Y**, applying **regularization**, and using **robust features**, the model:\n",
        "\n",
        "  * Gives **stable, interpretable coefficients**\n",
        "  * Reduces **overfitting** and **variance**\n",
        "  * Produces **reliable predictions** for pricing decisions and investment planning\n",
        "\n"
      ],
      "metadata": {
        "id": "-_6jZmOgjjno"
      }
    }
  ]
}